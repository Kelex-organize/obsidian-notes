Un sistema de $m$ ecuaciones lineales y $n$ incógnitas consiste de un conjunto de relaciones algebraicas de la forma
$$\sum_{j=1}^n a_{ij}x_j =b_i \quad \forall i=1 ...m,$$
que puede ser representado en notación matricial como $Ax = b$:
$$A = \begin{bmatrix} 
a_{11} & a_{12} & ... & a_{1n} \\
a_{21} & a_{22} & ... & a_{21} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & ... & a_{mn} \\
\end{bmatrix} \in M_{m \times n}(\mathbb R), \quad
\mathbf x = \begin{bmatrix} x_{1}\\ x_{2}\\ \vdots\\ x_{n}\\\end{bmatrix}
\in \mathbb R^n, \quad
\mathbf b = \begin{bmatrix} b_{1}\\ b_{2}\\ \vdots\\ b_{m}\\\end{bmatrix}
\in \mathbb R^m
$$
**Obs:** Si $n = m$, es una matriz cuadrada, y tiene solución sí y solo sí el $det(A) \neq 0$.
### Métodos de resolución
- **Directos:** encuentran la solución en un número finito de pasos (si no hay errores de redondeo).
- **Indirectos:** generan una sucesión de aproximaciones que converge a la solución.

# Escalerización Gaussiana
El método de escalerización gaussiana es un método directo para resolución de sistemas de ecuaciones lineales. Este algoritmo permite llevar un sistema $A \mathbf x = \mathbf b$, con $A$ cuadrada y no singular, a uno equivalente de tipo triangular superior.
### Sistemas triangulares
Un sistema triangular superior es de la forma:
$$Ux = b, \quad U = \begin{bmatrix} 
u_{11} & u_{12} & ... & u_{1n} \\
0 & u_{22} & ... & u_{21} \\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & ... & u_{mn} \\
\end{bmatrix}, x,b \in \mathbb R^n $$
Donde las soluciones de este problema son:
- Si $u_{ii} \neq 0$para todo $i$, la solución existe y es única.
- Si algún $u_{kk} = 0$:
	- Si la fila $k$ es $[0 ... 0]$ y $b_k \neq 0$ $\rightarrow$ no tiene solución.
	- Si la fila $k$ es $[0 ... 0]$ y $b_k = 0$ $\rightarrow$ tiene infinitas soluciones.
**Sustitución para atrás**
1. Se empieza con la última ecuacion
2. Luego se va hacia atás
3. el vector x es la solucion
```octave 
function x = sustitucion_hacia_atras(U, b)
	n = length(b);
	x = zeros(n,1);
	for i = n : -1: 1
		suma = 0;
		for j = i :n
			suma = suma + U(i,j) * x(j);
		end
		x(i) = (b(i) - suma) / U(i,i);
	end
end

U = [2 1 -1;
     0 3 2;
     0 0 4];
b = [3; 9; 8];
x = sustitucion_hacia_atras(A,b)'      # x = [1.6667 ,1.6667, 2.0000]
```
**Obs:** El costo computacional de la sustitución hacia atrás (o hacia adelante) es: 
$$\sum^n_{i = 1} (i - 1 + i - 1 + 1) = \sum^n_{i = 1} (2i - 1) = n^2$$
### Escolarización gaussiana (sin pivoteo) 
Sea A \in M_n(\mathbb R) una matriz no singular y b \in \mathbb R^n. Para llevar el sistema Ax = b a uno equivalente de tipo triangular superior, el algoritmo de escalerización gaussiana consiste en realizar operaciones elementales sucesivamente.

```octave
function [A, L, b] = escalerizacion_sin_pivoteo(A, b)
	n = length(b);
	L = eye(n);
	for k = 1 : n-1
		for i = k+1 : n
			L(i,k) = A(i,k) / A(k,k);
			A(i,k) = 0;
			for j = k+1 : n
				A(i,j) = A(i,j) - L(i,k) * A(k,j);
			end
			b(i) = b(i) - L(i,k) * b(k);
		end
	end
end

A = [2 1 1;
     4 3 3;
     8 7 9];
b = [2; 5; 12];
[A_tri, L, b_mod] = escalerizacion_sin_pivoteo(A, b);

disp("A_tri = ");
disp(A_tri);
disp("L = ");
disp(L);
disp("b_mod = ");
disp(b_mod');
```

### Escalerización gaussiana (con pivoteo parcial)


```octave
function [A, L, b] = escalerizacion_pivoteo_parcial(A, b)
    n = length(b);
    L = eye(n);
    for k = 1:n-1
        [~, p_rel] = max(abs(A(k:n, k)));
        p = p_rel + k - 1;

        b([k, p]) = b([p, k]);
        A([k, p], :) = A([p, k], :);

        for i = k+1:n
            L(i,k) = A(i,k) / A(k,k);
            A(i,k) = 0;
            for j = k+1:n
                A(i,j) = A(i,j) - L(i,k) * A(k,j);
            end
            b(i) = b(i) - L(i,k) * b(k);
        end
    end
end
A = [2 1 1;
     4 3 3;
     8 7 9];
b = [2; 5; 12];
[A_tri, L, b_mod] = escalerizacion_pivoteo_parcial(A, b);

disp("A_tri = ");
disp(A_tri);
disp("L = ");
disp(L);
disp("b_mod = ");
disp(b_mod');
```

# Descomposición $LU$
La descomposición $LU$ es un método que consiste en expresar una matriz cuadrada $A$ como el producto de una matriz triangular inferior $L$ y una triangular superior $U$. Esta factorización permite resolver sistemas lineales $Ax = b$ de forma más eficiente, ya que basta con resolver sucesivamente dos sistemas triangulares más simples.

Teorema: Sea $A \in M_n (\mathbb R)$ no singular.
 - Una matriz de permutación P.
 - Una matriz triangular inferior L, cuya diagonal está formada por unos.
 - Una matriz triangular superior U.
tales que vale la identidad $PA = LU$.

### Definiciones de matrices
- **Definición:** Una matriz $P \in M_n(\mathbb R)$ se dice de permutación si $P$ se puede obtener intercambiando filas o columnas de matriz identidad $Id \in M_n (\mathbb R)$.
	- Aplicar P a una matriz es equivalente a reordenar las filas.
	- Las matrices de permutación son ortogonales: $P_{-1} = P^T$.
- **Definición:** Una matriz de multiplicadores es una matriz $M = (m_{ij}) \in M_n (\mathbb R)$ triangular inferior, con unos en la diagonal, tal que:
	- Debajo de la diagonal tiene elementos distintos de cero solo en una columna.
	- Cumple que $|m_{ij}| \le 1$ para todo $i,j$.
# Matrices Dispersas y de Banda
El algoritmo de escalerización gaussiana suele tener un costo de O(\frac{2n^3}{3}) flops en general. 
Factorización de Cholesky para matrices simetricas y definidad positivas

### Matriz Dispersa
**Definición:** La dispersidad de una matriz $A \in M_n (\mathbb R)$ es la fracción de sus elementos que son cero. Diremos que una matriz $A \in M_n(\mathbb R)$ es dispersa si su dispersidad es cercana a 1.

Las matrices dispersas como tiene muchos ceros, siendo no eficiente almacenarlos. La técnica para almacenar este tipo de matrices es solo considerar los elementos no nulos.

### Matriz de Banda
**Definición:** El ancho de banda de una matriz $A \in M_n(\mathbb R)$ es la máxima distancia entre sus elementos no nulos y la diagonal.

A las matrices tridiagonales es posible realizarles una factorización LU en forma extremadamente eficiente, computando matrices L triangulaes inferior y U triangulares superior y ambas con el ancho de banda igual a 1. Algoritmo de Thomas, consto computacional O(n).

# Estabilidad y Convergencia
 




# Ejercicios
### Ejercicio 7
Consideremos el siguiente problema: hallar $y: [a,b] \rightarrow \mathbb R$ tal que
$$\tag E \begin{cases}
y^{′′}(x) + g(x)y(x) = f(x) \quad \text{para }x ∈ (a, b)\\
y(a) = α, y(b) = β
\end{cases}$$
donde $f,g : (a,b) \rightarrow \mathbb R$ son funciones conocidas. Se desea hallar una aproximación numérica de la solución de $(E)$.
1. Dividir el intervalo $[a,b]$ en $N$ subintervalos de largo $h = \frac{b-a}{N}$ y tomar como incógnitas los valores de $y$ de los puntos de subdivisión interiores a $[a,b]$. Éstos son de la forma $y_i = y(x_i)$, $i= 1, ... , N-1$, con
$$x_i = a + ih, \qquad i=0, ... , N$$
En los extremos del intervalo $x_0 =a$ y $x_N =b$ la función $y$ es conocida: $y_0 = y(x_0) = y(a) = \alpha$ e $y_N = y(x_N) = y(b) = \beta$.
**Solución:**

2. Para $i = 1, ... , N-1$ considerar la aproximación
$$y'' (x_1) \approx \frac{y_{i+1} - 2y_i + y_{i-1}}{h^2}$$
Usando desarrollo de Taylor alrededor de $x_i$, y asumiendo que la función $y$ es tan regular como sea necesario, mostrar que el error de aproximación es $O(h^2)$.
**Solución:**

3. Imponer la ecuación $(E)$ en cada uno de los puntos $x_i$ para $i= 1,...,N-1$ y obtener un sistema lineal de ecuaciones
$$yi−1 + (gih 2 − 2)yi + yi+1 = fih 2 , i = 1, . . . , N − 1$$
donde $f_i = f(x_i)$ y $g_i=g(x_i)$ para $i=1,...,N-1$, y además $y_0 = \alpha, y_n = \beta$.
**Solución:**

4. Escribir el sistema anterior en forma matricial $Ax = b$ con $A$ de dimensiones $(N −1)×(N −1)$, y $x$, $b$ de dimensiones $(N − 1) × 1$. Resolver en Octave el problema $(E)$ con
$$a=0, b=5, \alpha=0, \beta = \sin(5), f(x) = \sin(x)(e^x - 1), g(x) = e^x$$
**Solución:**

```octave
```

5. Para $N = 50$, graficar el resultado obtenido y compararlo con la solución exacta $y(x) = \sin(x)$
**Solución:**

---
### Ejercicio 12
Las cadenas de Markov modela sistemas que transitan entre un conjunto finito de estados según probabilidades de transición. Si $P \in M_n(\mathbb R)$ es la matriz de transición (es una matriz con entradas no negativas, en las que cada fila suma $1$), la distribución estacionaria $\pi$ satisface
$$\pi = \pi P, \qquad \sum^n_{i=1}\pi_i = 1$$
Si escribimos $\mathbf x = \pi^t$ como el vector columna, obtenemos
$$(I − P^t ) \mathbf x = 0, \qquad  1^t\mathbf x = 1,\qquad 1^t = [1 \cdots 1]$$
Considerar la cadena de Markov de $4$ estados y matriz de transición
$$ P = \begin{pmatrix}
0,5 && 0,2 && 0,2 && 0,1 \\ 
0,3 && 0,4 && 0,2 && 0,1 \\ 
0,2 && 0,3 && 0,4 && 0,1 \\ 
0,1 && 0,1 && 0,2 && 0,6
\end{pmatrix}$$

donde las filas corresponden al estado actual y las columnas el estado siguiente.
1. ¿Qué dificultad surge al intentar resolver el sistema $(I - P^t)\mathbf x = 0$ con un método directo?
**Solución:**

2. Una estrategia para obtener los estados estacionarios consiste en remplazar una de las ecuaciones del sistema por la condición de normalización $1^t \mathbf x = 1$. Escribir el sistema lineal para el vector estacionario $\mathbf x = (x_1, x_2, x_3,x_4)^t$ usando $(I-P^t)\mathbf x = 0$ y las restricción $x_1 + x_2 + x_3 + x_4 = 1$. Mostar la matriz $4 \times 4$ completa y el lado derecho.
**Solución:**

3. Resolver el sistema computacionalmente
**Solución:**

4. Verificar que, siendo $\mathbf x$ la solución obtenida, el vector fila $\pi = \mathbf x^t$ cumple $\pi P = \pi$ dentro del error de redondeo, que sus entradas suman $1$ y que son no negativas.
**Solución:**