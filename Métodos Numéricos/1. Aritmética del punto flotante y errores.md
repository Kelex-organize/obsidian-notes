# 1.1 Errores
La noción de error se refiere a una discrepancia entre lo que computamos y lo que queremos computar. Para estimar el error cometido al aproximar una cierta cantidad, es importante distinguir la magnitud absoluta del error de su magnitud relativa.

**Definición:** Sean $x \in \mathbb{R}$ un número real dado y $\bar x \in \mathbb{R}$ una aproximación de . 
- **Error absoluto:** Definimos el error absoluto como la diferencia entre $x$ y $\bar x$
$$e_x := \bar x - x \quad \Longrightarrow \quad \bar x = x + e_x$$
- **Error relativo:** Definimos el error relativo como el error absoluto normalizado por $x$:
$$\varepsilon_x := \frac{\bar x - x}{x} = \frac{e_x}{x} \quad \Longrightarrow \quad \bar x = (1 + \varepsilon_x) x$$

**Obs:** Las definiciones anteriores implican que los errores tienen signos, pero en general vamos a trabajar con el modulo del error ($|e_x|$ y $|\varepsilon_x|$).
### Número de condición
Sea $f$ una función derivable que podemos computar exactamente. Si  es una aproximación de $x$, el error relativo en $f(x)$ puede estimarse con un desarrollo de Taylor:
$$\varepsilon_y  = \frac{f(\bar x) - f(x)}{f(x)}\approx \frac{f'(x)(\bar x - x)}{f(x)} = \frac{f'(x)x}{f(x)}\frac{\bar x - x}{x} \quad \Longrightarrow \quad \varepsilon_y \approx \frac{f'(x)x}{f(x)} \cdot \varepsilon_x$$
**Definición:** Llamamos número de condición de $f$ en el punto $x$ al número:
$$k_f(x) := \frac{f'(x)x}{f(x)}$$
- Si $k_f(x) \sim 1$, diremos que el problema de evaluar $f$ en $x$ está bien condicionado.
- Si $k_f(x) \gg 1$, diremos que el problema de evaluar $f$ en $x$ está mal condicionado.
### Propagación de errores en operaciones aritméticas
Sean $x$ e $y$ dos números reales, para los cuales tenemos almacenado aproximaciones respectivas $\bar x$ e $\bar y$. Analizaremos la propagación de errores al realizar operaciones aritméticas elementales:
**Suma y Resta:** $\varepsilon_{x + y} = \frac{(\bar x + \bar y) - (x + y)}{x+y} = \frac{x}{x+y}\varepsilon_x + \frac{y}{x + y}\varepsilon_y$
**Producto:**  $\bar x \bar y = xy(1 + \varepsilon_x) (1 + \varepsilon_y) \approx xy (1 + \varepsilon_x + \varepsilon_y) \quad \Longrightarrow \quad \varepsilon_{xy} \approx \varepsilon_x + \varepsilon_y$ 
**División:**  $\frac{\bar x}{\bar y} = \frac{x(1 + \varepsilon_x)}{y(1 + \varepsilon_y)} \approx \frac{x}{y}(1 + \varepsilon_x)(1 - \varepsilon_y) \approx \frac{x}{y}(1 + \varepsilon_x - \varepsilon_y) \quad \Longrightarrow \quad \varepsilon_{x/y} \approx \varepsilon_x - \varepsilon_y$
### Cancelación Catastrófica
Observemos que si $x \approx -y$ ($x + y \approx 0$), el error relativo de la Suma o Resta se vuelve muy grande. A este fenómeno se le llama como **cancelación catastrófica**.
¡Cuando nos encontramos con problemas que implican restar números de igual magnitud, es importante buscar reescribir las fórmulas para evitar cancelaciones catastróficas!
# 1.2 Aritmética del punto flotante
La aritmética de punto flotante es el sistema mediante el cual las computadoras representan números reales de forma aproximada usando una cantidad finita de bits. Debido a esta limitación, no todos los números reales pueden representarse exactamente, y las operaciones están sujetas a errores de redondeo.
### Representación de punto flotante
En la aritmética de punto flotante los números son almacenador por un conjunto de bits. El estándar es la representación de precisión doble de la IEEE $754$.
- $1$ bit para el signo ($\pm$). 
- $52$ bit para la mantisa ($f$).
- $11$ bits para el exponente ($e$).
Así que los números de punto flotante son aquellos que se pueden escribir de la forma
### Constantes de máquina y números especiales
- **Épsilon de máquina:** es la separación entre el número $1$ y el siguiente número representable en el sistema de punto flotante. $\varepsilon_m = 2^{-52} \approx 2,2 \times 10^{-16}$
- **Real máximo:** es el mayor número representable en un sistema de punto flotante. $\mathbf {Real}_{max} = (2 - 2^{52}) \times 2^{1023} \approx 1,8 \times 10^{308}$
- **Real mínimo:** es el menor número representable en un sistema de punto flotante. $\mathbf{Real}_{min} = 1 - 2^{1022} \approx 2,2 \times 10^{-308}$
- **Overflow:** El resultado de un calculo que produce un número mayor que $\mathbf{Real}_{max}$. Se almacena con el exponente $e = 1024$ y la mantisa $f=0$. $\mathbf{Inf}$
- **Indefinido:** si hacemos un calculo que no esta definido en un sistema real. Se almacena con el exponente $e = 1024$ y una mantisa $f \neq 0$. $\mathbf{NaN}$
- **Cero:** El número cero en el formato IEEE 754, se tiene con signo y se tratan como objetos distintos. Se representa con el exponente $e = -1023$, y la mantisa $f = 0$ y el signo determina de qué cero se trata.
- **Subnormales:** Puede ocurrir que hagamos que la máquina un cálculo cuyo resultado es un número real menor al valor absoluto d. Es este caso, 
### Error de representación en punto flotante
Como la representación de número flotante no puede abarcar todos los números, hay representaciones en las cuales es necesario realizar redondeos, por lo cual hay que considerar el error de aproximación de la representación.

**Prop:** Sea $x \in \mathbb R$ tal que $2^{-1022} \le |x| < 2^{1024}$ y $fl(x)$ su representación de punto flotante. Entonces, se cumple:
$$|\varepsilon_x| = \frac{fl(x) - x}{|x|} \le \frac{\epsilon_M}{2} \quad \Longrightarrow \quad |e_x| = |x \varepsilon_x| \le \frac{|x|\varepsilon_M}{2}$$
¡Mientras nuestra mejor cota para el error relativo es aproximar un número real es uniforme, nuestra mejor cota para el error absoluto depende de la magnitud de dicho número real!

# 1.3 Propagación de errores en punto flotante
La aritmética de punto flotante no es asociativa ni distributiva, las operaciones se hacen por etapas y en cada operación se aplica el redondeo correspondiente. Esto introduce pequeños errores de redondeo que se acumulan y se propagan a lo largo del cálculo.
### Error hacia adelante
Sea $f: I \rightarrow \mathbb R$ una función derivable que podemos computar exactamente, y $x \in I$ un punto tal que $f(x) \neq 0$. En la práctica, trabajamos con la representación en punto flotante $\text{fl}(x)$, con un error relativo:
$$\frac{\text{fl}(x) - x}{x} \le \frac{\varepsilon_M}{2}$$
Suponiendo que logramos computar $\hat y = f(\text{fl}(x))$ y luego aproximamos el resultado usando punto flotante $\text{fl}(\hat y)$, obtenemos un error relativo:
$$\frac{\hat y - y}{y} \le k_f(x)\frac{\varepsilon_M}{2}$$
donde $k_f(x) := \frac{f'(x)x}{f(x)}$ número de condición de f en el punto $x$.
Al considerar el error relativo acumulado al finalizar el proceso, obtenemos que:
$$\frac{\text{fl} (\hat y) - y}{y}  \le \frac{\text{fl}(\hat y) - \hat y}{y} + \frac{\hat y - y}{y} \approx \frac{\text{fl}(\hat y) - \hat y}{\hat y} + \frac{\hat y - y}{y} \le \frac{\varepsilon_M}{2} + k_f(x)\frac{\varepsilon_M}{2}$$
Llamamos **error inevitable al computar** $f(x)$ al termino $\frac{\varepsilon_M}{2} + k_f(x)\frac{\varepsilon_M}{2}$. Es la mejor cota que podemos obtener para el error relativo al computar $f$ exactamente. Para saber si un algoritmo es "bueno", el error relativo de la salida es comparable con el error inevitable.
### Calculo de derivadas usando diferencias hacia adelante



**Error de truncamiento:** 

**Error de punto flotante:**

$$\text{fl}(f(x + h)) = (1 + \varepsilon_{f(x+h)})f(x+h) , \quad \text{fl}(f(x)) = (1 + \varepsilon_{f(x)})f(x), \quad || \le \frac{\varepsilon_{f(x+h)}}{}$$